<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://windchang1999.github.io</id>
    <title>windchang1999&apos;s blog</title>
    <updated>2021-08-02T03:58:37.496Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://windchang1999.github.io"/>
    <link rel="self" href="https://windchang1999.github.io/atom.xml"/>
    <logo>https://windchang1999.github.io/images/avatar.png</logo>
    <icon>https://windchang1999.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, windchang1999&apos;s blog</rights>
    <entry>
        <title type="html"><![CDATA[ON-DEMAND-FORK: A Microsecond Fork for Memory-Intensive and Latency-Sensitive Application]]></title>
        <id>https://windchang1999.github.io/post/on-demand-fork-a-microsecond-fork-for-memory-intensive-and-latency-sensitive-application/</id>
        <link href="https://windchang1999.github.io/post/on-demand-fork-a-microsecond-fork-for-memory-intensive-and-latency-sensitive-application/">
        </link>
        <updated>2021-08-01T02:02:22.000Z</updated>
        <summary type="html"><![CDATA[<p>内存密集场景下的一中 fork 的优化实现。EuroSys'21, from Purdue University</p>
]]></summary>
        <content type="html"><![CDATA[<p>内存密集场景下的一中 fork 的优化实现。EuroSys'21, from Purdue University</p>
<!-- more -->
<h1 id="1-introduction">1. Introduction</h1>
<p>在通常的 fork 实现中，通过 COW 机制避免了父进程所有物理页的复制，但仍然需要将进程相关的内核状态全部复制，例如页表、文件描述符及 namespace。on-demand-fork 将 COW 机制拓展到最后一级页表，避免了内存密集型应用在 fork 过程中复制最后一级页表的开销。</p>
<h1 id="2-background-motivation">2. Background &amp; Motivation</h1>
<p>在快照（Redis），测试（AFL）及虚拟机测试等内存密集型场景下，执行 fork 系统调用需要花费大量的时间。当前 fork 的实现，存在以下两点不足：</p>
<ol>
<li>fork 复制的页表的数量与其所占用的内存大小约成正比。复制大量页表会带来一定程度的延迟，例如在 1GB 内存占用延迟为 6.5ms，50GB 内存占用延迟为 253.9ms。</li>
<li>在多个进程并发使用 fork 系统调用时，由于需要原子性地更新物理页的引用计数，因此存在竞争关系，并发拓展性较差。例如3个进程同时执行 fork 时 1GB 内存占用对应的延迟为 22.4 ms。<br>
<img src="https://windchang1999.github.io/post-images/1627784999191.png" alt="" loading="lazy"><br>
一种解决方案是使用 huge page，但其存在内部碎片大，缺页中断开销大，并且会降低子进程和父进程共享物理页的机会等缺陷。这些特性都会增加程序的 tail latency。<br>
在 on-demand-fork 过程中，子进程与父进程共享最后一级页表，将最后一级页表的复制工作，以及物理页引用计数的更新，推迟到需要写入内存或是需要修改进程的虚拟内存空间时执行。从而降低 fork 调用的延迟，并提高并发性能。</li>
</ol>
<h1 id="3-implementation">3. Implementation</h1>
<figure data-type="image" tabindex="1"><img src="https://windchang1999.github.io/post-images/1627787173054.png" alt="" loading="lazy"></figure>
<h2 id="31-overview">3.1 Overview</h2>
<p>修改内容：</p>
<ol>
<li>为最后一级页表添加一个引用计数器</li>
<li>修改 page fault handler，除了复制对应的数据物理页，还需要复制最后一级页表，并修改相关页表及数据物理页的引用计数器</li>
<li>修改物理页的 Free 逻辑，对一个引用计数为1的物理页的 free 操作，必须检查其页表的引用计数</li>
<li>修改 munmap、mremap 等函数的逻辑，在改变进程的虚拟地址空间映射时，如果涉及到的最后一级页表处在共享状态，则必须对最后一级页表采取相应的操作</li>
</ol>
<h2 id="32-fork-过程中页表复制流程">3.2 fork 过程中页表复制流程</h2>
<p>如上图，在子进程中建立前三级页表，第三级页表的内容则复制于父进程，使得 PMD 指向同一个第四级页表，更新第四级页表的引用计数器。通过 MMU 提供的 hierachical attribute 特性，将第三级页表 PMD 的 writeable 位置 0 来标记整个第四级页表为不可写状态。<br>
在 on-demand-fork 过程中不涉及到物理页的引用计数的修改，因此提高了并发性能。</p>
<h2 id="33-对内存写引起-page-fault-的流程">3.3 对内存写引起 Page Fault 的流程</h2>
<p>如果共享最后一级页表的进程执行了写操作，那么 PMD 中的 writeable 位则会引起 page fault。在中断处理程序中，需要：</p>
<ol>
<li>复制最后一级页表，更新 PMD 指向新的最后一级页表，降低其引用计数</li>
<li>复制数据物理页，更新 PTE 指向新的数据物理页<br>
&lt;需要插入一张图片&gt;</li>
</ol>
<h2 id="34-unmapping-or-remapping-vmas">3.4 Unmapping or Remapping VMAs</h2>
<p>Unmapping 和 Remapping 需要修改进程的页表内容，因此如果修改过程中发现共享的最后一级页表，那么则需要执行相应的操作。以 munmap 为例：</p>
<ol>
<li>如果进程中仅有 1 个 VMA 使用了共享的最后一级页表，那么可以直接断开 PMD 到 PTE 的连接，降低最后一级页表的引用计数</li>
<li>如果进程中有多个 VMA 使用了共享的最后一级页表，那么则需要对最后一级页表执行 COW 操作<br>
如何检测两个 VMA 是否使用了同一个最后一级页表？最后一级页表映射了 page_size * NR_PTEs 的空间大小，比较两个 VMA 的地址的高位即可。</li>
</ol>
<h1 id="4-evaluation">4. Evaluation</h1>
<h2 id="40-setup">4.0 Setup</h2>
<p>CPU：16-core AMD EPYC 7302P CPU<br>
Memory：256 GB<br>
Linux Kernel Version：5.6.19</p>
<h2 id="41-microbenchmarks">4.1 Microbenchmarks</h2>
<h3 id="411-system-call-latency">4.1.1 System Call Latency</h3>
<p><img src="https://windchang1999.github.io/post-images/1627823295825.png" alt="" loading="lazy"><br>
因为不需要在 fork 过程中复制最后一级页表，因此 on-demand-fork 的执行时间更短。在 1GB 内存占用时所需时间约为 1/65，在 50GB 内存占用时所需时间约为 1/270。而 on-demand-fork 比使用了 huge page 的 fork 更快还与 huge page 需要在 fork 过程中给 PMD 加锁的原因。</p>
<h3 id="412-page-fault-handling-overhead">4.1.2 Page Fault Handling Overhead</h3>
<p><img src="https://windchang1999.github.io/post-images/1627824064954.png" alt="" loading="lazy"><br>
on-demand-fork 的缺页中断执行时间约为通常 fork 的 5.3 倍，后续的原因需要思考一下。因为在中断处理程序中只是需要多复制一个最后一级页表，以及需要设置最后一级页表的引用计数。可能与引用计数器需要使用原子指令进行操作有关。<br>
on-demand-fork 的缺页中断执行时间比使用 huge page 的 fork 短是自然的，毕竟 huge page 的缺页中断需要复制 2MB 的物理页，是 4KB 物理页的 512 倍。</p>
<h3 id="413-overall-performance">4.1.3 Overall Performance</h3>
<p><img src="https://windchang1999.github.io/post-images/1627824495354.png" alt="" loading="lazy"><br>
在本次测试中，应用占用了 50GB 的内存。<br>
访问内存的区域越多，访存操作中写操作越多，则在 on-demand-fork 中需要执行更多的最后一级页表及物理页的 COW 操作，摊平了on-demand-fork 带来的加速效果。在最坏情况，即需要写入所有的内存区域时，则几乎没有加速效果（仅 4%）。</p>
<h2 id="42-real-world-applications">4.2 Real-world Applications</h2>
<h3 id="421-fuzzing-afl">4.2.1 Fuzzing: AFL</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825367579.png" alt="" loading="lazy"><br>
吞吐量约有 2.2 倍的提升。</p>
<h3 id="422-unit-testing-sqlite">4.2.2 Unit Testing: SQLite</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825450356.png" alt="" loading="lazy"><br>
较大程度地降低了 fork 过程所使用的时间，使得主要开销移动到测试用例的运行中。</p>
<h3 id="423-snapshots-redis">4.2.3 Snapshots: Redis</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825534681.png" alt="" loading="lazy"><br>
latency 更低，波动更小，且对 tail latency 降低明显。</p>
<h3 id="424-triforceafl-vm-cloning">4.2.4 TriforceAFL: VM Cloning</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825589905.png" alt="" loading="lazy"><br>
大约同 4.2.1。</p>
<h3 id="425-apache-http-server-on-demand-fork-不能加速的例子">4.2.5 Apache HTTP Server （on-demand-fork 不能加速的例子）</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825634742.png" alt="" loading="lazy"><br>
因为 HTTP Server 并非内存大户（在测试中 Apache 仅用了 7MB 内存），因此 fork 过程中的瓶颈并不是页表的复制，因此 on-demand-fork 和 fork 的表现没有明显的区别。</p>
<h1 id="5-不足之处">5. 不足之处</h1>
<ol>
<li>论文中对 fork 的并发性能做了测试（3个相同的进程同时执行 fork），但没有对 on-demand-fork 做相同的测试</li>
<li>在 on-demand-fork 过程中只针对页表进行了 COW 操作。是否可以找一些其他的场景，将 COW 操作运用到 fork 过程中需要复制的其他内核变量上？</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Linux in Unikernel Clothing]]></title>
        <id>https://windchang1999.github.io/post/a-linux-in-unikernel-clothing/</id>
        <link href="https://windchang1999.github.io/post/a-linux-in-unikernel-clothing/">
        </link>
        <updated>2021-07-23T01:20:08.000Z</updated>
        <summary type="html"><![CDATA[<p>如何把 Linux 定制得和 Unikernel 一般？原文发表于 EuroSys'20，作者来自 UIUC 及 IBM。</p>
]]></summary>
        <content type="html"><![CDATA[<p>如何把 Linux 定制得和 Unikernel 一般？原文发表于 EuroSys'20，作者来自 UIUC 及 IBM。</p>
<!-- more -->
<h1 id="1-introduction">1. Introduction</h1>
<p>该团队针对仿照 Unikernel 的云计算场景，通过：</p>
<ul>
<li>Kconfig 系统对 Linux Kernel 做定制裁剪</li>
<li>借助 Kernel Mode Linux 补丁让用户态程序能够运行在内核态来减小系统调用的上下文切换开销<br>
最终实现了与 Unikernel 性能相近的一个 Linux 内核发行版，并且拥有更高的兼容性，鲁棒性及更低的应用开发成本。</li>
</ul>
<h1 id="2-background-motivation">2. Background &amp; Motivation</h1>
<p>在理想的微服务的场景下，每个虚拟机实例只运行单个进程，Unikernel 是针对微服务场景的一种高性能、低资源占用、高安全性的应用部署方案。Unikernel 的核心思想是通过 LibOS 的形式向用户态应用程序提供网络、文件系统、硬件驱动等操作系统服务，并将用户态程序与 LibOS 链接为单个二进制。<br>
对于当前的 Unikernel 技术，作者主要分成两类：</p>
<ol>
<li>Language-based<br>
特点是需要采用与 LibOS 一致的编程语言编写应用程序。比如最早的 Unikernel 框架 MirageOS 要求开发者使用 OCaml 语言，IncludeOS 框架则要求使用 C++ 语言。因此这种方案有可能存在需要用特定编程语言重写软件的可能。</li>
<li>POSIX-like<br>
特点是需要重新实现一套 POSIX 标准，缺点在于工程量太大，实现难度高，兼容性差，而且往往在重新实现 Linux （至少在重新实现一部分 Linux）。虽然有部分框架例如 Rumprun 重用了 NetBSD 的代码，减小了工程量，提高了兼容性，但存在定制程度不够高的问题。<br>
这两种方案各有优缺点，但作者认为统一的问题在于无法重用 Linux 及开源社区社区多年来的成果，重复造轮子，并且部署难度高（Language-based）或者是框架开发难度高（POSIX-like），兼容性无法保证。<br>
作者认为理想的应用部署方案应该既有 Unikernel 的高性能、低资源占用特性，又能够良好的与开源社区对接，针对以上痛点，因此提出了 Linux 高度定制这一条路线。</li>
</ol>
<h1 id="3-method">3. Method</h1>
<p><img src="https://windchang1999.github.io/post-images/1627217321610.png" alt="" loading="lazy"><br>
整体而言，本文中的方案有两部分工作，分别为：</p>
<ul>
<li>Specialization：通过对内核进行高度的定制、裁剪，同时定制 init 脚本及部分 rootfs 的内容，减小生成二进制的大小，提高启动速度。</li>
<li>System call overhead elimination：对内核和 musl libc 添加 KML 补丁，保证内核和应用程序都运行在内核态上，以此减小系统调用引起的上下文切换的开销。<br>
在本方案中，Linux Kernel 采用 4.0 版本，rootfs 则从 Docker Hub 中获取。</li>
</ul>
<h2 id="31-specialization">3.1 Specialization</h2>
<p><img src="https://windchang1999.github.io/post-images/1627218849240.png" alt="" loading="lazy"><br>
<img src="https://windchang1999.github.io/post-images/1627217796115.png" alt="" loading="lazy"><br>
本文中针对内核的裁剪在 Kconfig 的导引下完成。作者并没有遍历16000多个 Linux 配置项，而是从 AWS FIrecracker 的 microVM 的833个配置项开始，逐个地手动剔除配置项，最终剩下283个配置项，构成了 lupine-base config。<br>
在剔除出的配置项中，作者将其分为三类，分别是：</p>
<ol>
<li>Application speicific：例如某些系统调用，一些比较小众的网络协议及文件系统，一些压缩及加密算法实现，Debugging-related 等</li>
<li>Multiprocessing &amp; 安全相关：例如 SMP/NUMA，cgroup/namespace；SELINUX，KPTI（kernel page table isolation）</li>
<li>HW management：driver，arch-specific，power-management<br>
作者认为2、3属于在 Unikernel 场景下属于不必要的配置项，而1中配置项则需要根据部署的应用不同，按需开启。利用 lupine-base config 加上适当的 Application speicific 的配置项编译内核，即完成内核的定制。<br>
针对 init 脚本的定制在本文中没有详细描述，但大致是根据 kernel 定制的情况，按需屏蔽一些系统服务的启动流程。</li>
</ol>
<h2 id="32-eliminating-system-call-overhead">3.2 Eliminating System Call Overhead</h2>
<p>主要是使用了 Kernel Mode Linux 补丁，这是一个现成的项目，其主要作用是允许原来运行在用户态的应用程序运行在内核态。原有的 KML 适用于多进程环境，仅有一部分进程运行在内核态（在 /trusted 目录下），而剩下的进程仍然运行在用户态。由于在 Unikernel 场景下，仅需要运行单个进程，所以作者对 KML 进行了修改，使所有用户态程序都运行在内核态下。<br>
此外，作者还针对 musl libc （的二进制）进行了小量的修改，主要是将原有的系统调用指令替换为普通的函数调用指令，而 kernel 的系统调用函数符号通过 vsyscall 方法导出。</p>
<h1 id="4-evaluation">4. Evaluation</h1>
<p>在性能测试部分，作者主要给出三个结论：</p>
<ol>
<li>内核的裁剪十分重要对比于 microVM，image size 减小 73%，启动时间提高 59%，内存占用减小 28%，吞吐量提高 33%；但是针对每个应用程序对内核做进一步的定制相比于 lupine-general 仅有4%的性能提升。只需要在 lupine-base 的基础上加入19个配置项就能运行 Docker Hub 上的 top 20 应用程序。</li>
<li>系统调用的额外开销不是 Unikernel 的主要性能瓶颈</li>
<li>相比于 POSIX-like 的 Unikernel，Lupine 拥有更好的泛用性，应用程序无需修改源码即可直接运行，并且能够重用 Linux 及 glibc 中高度优化的代码</li>
</ol>
<h2 id="41-configuration-diversity">4.1 Configuration Diversity</h2>
<p><img src="https://windchang1999.github.io/post-images/1627264288724.jpg" alt="" loading="lazy"><br>
上图展示了为了在 Lupine Linux 上运行 Docker Hub top 20 软件，所需要向 lupine-base 中加入的配置项的数量。<br>
本文中衡量软件是否正常运行的标准比较 ad hoc，主要是逐个加入配置项，然后观察软件的运行状态（例如基本功能是否正常，日志是否有异常输出等）。<br>
<img src="https://windchang1999.github.io/post-images/1627264539058.jpg" alt="" loading="lazy"><br>
上图说明为了运行 Docker Hub top 20 软件，只需要向 lupine-base 中加入 19 个配置项即可，作者定义 lupine-base + 这19个配置项 = lupine-general。因此针对单个软件定制的内核实际上和 lupine-general 仅仅只差了几个配置项，理论上性能不应该有太大的差距，后续环节也证明了这一结论。</p>
<h2 id="42-image-size">4.2 Image Size</h2>
<p><img src="https://windchang1999.github.io/post-images/1627264868930.jpg" alt="" loading="lazy"><br>
上图中只显示了 Kernel image size，lupine-base 的内核体积约 4MB。但这里都是和 POSIX-like 的 unikernel 来做对比，实际上 language-based 的 unikernel 能做到非常小，例如 MirageOS 框架编写的 DNS 服务器，内核+应用程序的总大小仅为 184KB。</p>
<h2 id="43-boot-time">4.3 Boot Time</h2>
<p><img src="https://windchang1999.github.io/post-images/1627265302276.jpg" alt="" loading="lazy"><br>
启动时间为 30ms 左右，这里作者使用的 lupine-nokml 是没有安装 KML 补丁的内核源码 + 开启了 CONFIG_PARAVIRT 配置项的变种，主要问题是 CONFIG_PARAVIRT 能够在 AWS Firecracker 环境下提高启动速度，但是会和 KML 补丁冲突，在后续环节可以看到是否安装 KML 对性能的影响不大。在使用 lupine-base 设置时启动时间会达到 71ms，比 microVM（开启了 CONFIG_PARAVIRT）更高。</p>
<h2 id="44-memory-footprint">4.4 Memory Footprint</h2>
<p><img src="https://windchang1999.github.io/post-images/1627265824051.jpg" alt="" loading="lazy"><br>
主要测试方法是逐渐降低 Firecracker 给 VM 提供的内存，直到系统工作异常。比较有趣的一点是 Linux-base 的 VM（如 microVM 和 lupine），无论运行何种软件占用的内存都是一个恒定值，这与 Linux 内核的内存 lazy-allocation 有关。</p>
<h2 id="45-system-call-latency-kml-improvement">4.5 System Call Latency &amp;&amp; KML improvement</h2>
<p><img src="https://windchang1999.github.io/post-images/1627266322347.jpg" alt="" loading="lazy"><br>
测试中的 null 表示 getpid 系统调用，read 表示对 /dev/zero 进行读操作，write 表示对 /dev/null 进行写操作。lupine-base 的 system call latency 明显更低，但有一点需要思考：为什么仅仅对内核进行剪裁（lupine-nokml 与 microVM 对比）能够明显降低 read/write system call latency，对内核的剪裁应该不会改变 system call 的调用路径，作者原文中没有给出解释。个人猜测应该与文件系统的性能、安全相关功能的禁用有关。</p>
<h2 id="46-application-performance">4.6 Application performance</h2>
<p><img src="https://windchang1999.github.io/post-images/1627266782521.jpg" alt="" loading="lazy"><br>
相比于 microVM 均有 20% 左右的性能提升，作者将其归功于关闭了一些安全功能，提高了性能。此外关注以下两点：</p>
<ol>
<li>lupine 和 lupine-genreal 对比，表明在应用程序这个层次做定制性能提升不大</li>
<li>lupine 和 lupine-nokml 对比，表明 KML 对性能影响不大，主要性能提升来自内核定制</li>
<li>其他 POSIX-like 的 unikernel 的系统服务代码由于缺乏充分的优化，因此性能甚至和 microVM 有一定的差距</li>
</ol>
<h1 id="5-beyond-unikernels">5. Beyond Unikernels</h1>
<p>无论是 language-based 还是 POSIX-like 的 unikernel，都存在泛用性问题。例如不能支持 fork，不能支持多处理器调用，在一些情况下会给 Linux 应用的部署带来困难。作者在这一章中着重展示了 Lupine 拥有更好的泛用性。向 lupine-base 加入多进程相关配置项，即可开启多进程支持，并且验证了其额外开销较小的结论。<br>
<img src="https://windchang1999.github.io/post-images/1627270504573.png" alt="" loading="lazy"><br>
首先是 control process （例如 shell、deamon 等不会和应用程序抢占资源的进程）并不影响 system call latency，这表明加入多个地址空间支持对内核的影响并不大（没有向内核中增加安全增强功能大）。<br>
<img src="https://windchang1999.github.io/post-images/1627270708424.png" alt="" loading="lazy"><br>
其次作者比较了在没有加入多进程支持时单地址空间下使用多个pthread线程，在加入多进程支持后使用多个进程两种情况下的 messaging benchmark。结果表明加入多进程支持对相互竞争资源的进程性能也不会有太大的影响。理论上讲，在 Linux 中 pthread 库创建的线程也采用 task_struct 结构体作为控制块，两种情况的差异只是是否切换页表基地址寄存器及 flush TLB 而已。<br>
此外，作者还针对 SMP 配置做了一定的测试，打开内核的 SMP 功能后，在 sem_posix, futex 和 make -j 三种场景下，性能损失分别为 3%、8%、3%。</p>
<h1 id="6-discussion">6. Discussion</h1>
<ol>
<li>通过人工的方法分析 Kconfig 的各个配置项可能存在误分类，同时不同的配置项可能有着类似的功能，能够相互替换，因此 lupine-base 并不唯一且最小</li>
<li>如何自动化地、准确地通过应用程序的源码或者二进制得到所需的内核配置项，是接下来要继续研究的问题。对于 language-based 的 Unikernel 不存在这一问题</li>
<li>lupine 没有实现轻量化的 VMM</li>
<li>lupine 不能实现“immutable infrastructure”，在应用部署上存在局限性，并且相比于 language-based 的 unikerel 攻击面更大（有 rootfs，有 loader，应用程序与OS没有链接为一个整体）</li>
<li>相比于 lannguage-based 的 unikernel ，lupine 不能实现跨应用程序和OS的程序优化</li>
</ol>
<h1 id="参考资料">参考资料</h1>
<ol>
<li>https://github.com/cetic/unikernels：一个介绍 unikernels 的项目，总体看来 unikernel 的性能可能不如 container</li>
<li>https://learning.oreilly.com/library/view/unikernels/9781492042815/：电子书unikernls</li>
</ol>
]]></content>
    </entry>
</feed>