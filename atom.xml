<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://windchang1999.github.io</id>
    <title>windchang1999&apos;s blog</title>
    <updated>2021-09-03T07:53:09.540Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://windchang1999.github.io"/>
    <link rel="self" href="https://windchang1999.github.io/atom.xml"/>
    <logo>https://windchang1999.github.io/images/avatar.png</logo>
    <icon>https://windchang1999.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, windchang1999&apos;s blog</rights>
    <entry>
        <title type="html"><![CDATA[Learning-based Memory Allocation for C++ Server Workload]]></title>
        <id>https://windchang1999.github.io/post/learning-based-memory-allocation-for-c-server-workload/</id>
        <link href="https://windchang1999.github.io/post/learning-based-memory-allocation-for-c-server-workload/">
        </link>
        <updated>2021-08-08T11:40:09.000Z</updated>
        <summary type="html"><![CDATA[<p>利用 LSTM 预测变量的寿命，结合新型的内存分配器缓解 C++ 服务器的 Huge Page 的内部碎片问题。ASPLOS'20, from Google &amp; CMU &amp; SBU。</p>
]]></summary>
        <content type="html"><![CDATA[<p>利用 LSTM 预测变量的寿命，结合新型的内存分配器缓解 C++ 服务器的 Huge Page 的内部碎片问题。ASPLOS'20, from Google &amp; CMU &amp; SBU。</p>
<!-- more -->
<h1 id="1-introduction">1. Introduction</h1>
<p>服务器的 memory footprint 变大，但 TLB 并没有随之增大，导致地址翻译的性能下降。一种解决方案是使用 huge page 来增加 TLB reach，以提高地址翻译性能。但缺乏管理的 huge page 几乎不可避免地会引入内部碎片，导致物理内存的浪费。<br>
作者指明了在长期运行的服务器中造成 huge page 的根本原因：即将寿命长短不一的对象分配于同一个 huge page 中。因为 C++ 的 allocator 并不会在运行过程中移动对象以整理碎片，当寿命较短的对象释放后，寿命长的对象仍然存在于 huge page 中，使得 huge page 被占用，留下大量的内部碎片，增加了整个服务器应用程序的内存占用量。<br>
long-lived object 占用 huge page 是十分常见的。即使假设 short-lived object 占总 object 数量的 99.99%，object size = 64 B 的情况下，在 2MB 的 huge-page 中至少存在一个 long-lived object 的概率则是 1 - (0.9999)^(2MB/64B) = 96%。因此使用 huge page 不可避免地会导致碎片问题。<br>
为了从根本上解决 huge page 的碎片问题，必须考虑以下要素：</p>
<ol>
<li>如何预测对象的寿命</li>
<li>如何尽可能将具有相似寿命的对象分配在同一个 huge page 中</li>
<li>在预测寿命存在偏差时，如何保证分配机制的效率<br>
同时，为了保证方案具有工程意义，还需要考虑运行的 overhead。</li>
</ol>
<h2 id="contribution">Contribution</h2>
<ol>
<li>使用 RNN 来对变量的寿命进行预测。该模型可以泛化到不同版本、不同构建方式的同一 Server Application 中</li>
<li>设计了 Learned Lifetime-Aware Memory Allocator （LLAMA），实现了基于 object lifetime classes 的 huge page 内部碎片的优化</li>
</ol>
<h1 id="2-motivation-background">2. Motivation &amp; Background</h1>
<h2 id="21-server-segmentation">2.1 Server Segmentation</h2>
<p><img src="https://windchang1999.github.io/post-images/1628427362253.png" alt="" loading="lazy"><br>
<img src="https://windchang1999.github.io/post-images/1628427369625.png" alt="" loading="lazy"><br>
在 Server 运行过程中其实 long-lived object 占比很少，但使用 2MB 的 huge page 仍然会造成近2.15倍的内存占用。这是因为在本节的例子中，Server 在处理一个 request 时，除了需要动态分配一系列临时对象用来处理数据外，还需要分配一些 long-lived 的变量来记录系统状态。在这样的情形下使用 huge page 会导致大量内存碎片。<br>
当下已经有很多针对 4KB 页的内部碎片解决方案，但不适宜迁移到 huge page 中；降低 huge page 的内部碎片是更有挑战性的问题。</p>
<h2 id="22-lifetime-prediction-challenges">2.2 Lifetime Prediction Challenges</h2>
<p>先前的工作通常只是通过精确匹配堆栈上下文来二分地预测变量的生命周期，即给出一个 stack trace，预测器输出该变量属于 long-lived or short-lived，粒度过粗。而 LLAMA 则以 lifetime classes（LC）的粒度来管理变量，即将变量的寿命以对数划分为 1ms, 10ms, 100ms, 1s ....</p>
<h3 id="221-overhead">2.2.1 Overhead</h3>
<p><img src="https://windchang1999.github.io/post-images/1628428571014.png" alt="" loading="lazy"><br>
在运行过程中记录 stack trace 是相当耗时的，往往超过分配内存所需要的时间，再加上 prediction 所需要的时间，在运行过程中执行 lifetime prediction 会带来大量的 overhead。</p>
<h3 id="222-coverage-and-accuracy">2.2.2 Coverage and Accuracy</h3>
<p>作者在这一段拿 Profile-Guide Optimization(PGO) 和 lifetime prediction 所需要的 profile data 相比，大概说明的意思是 lifetime prediction 需要记录 allocation 和 free 时的 stack trace，数据量相对于 PGO 更多。这里不太懂 PGO 的原理，先留个坑。</p>
<h3 id="223-instability">2.2.3 Instability</h3>
<p>由于存在地址随机化和不同的编译配置，同一代码相同位置的 stack trace 也有可能不同，想要精确匹配 stack trace 存在相当的难度。因此当前的 lifetime predictor 通常是 online 进行 profile，带来比较严重的 overhead 问题。</p>
<h1 id="3-overview-of-lifetime-prediction">3. Overview of Lifetime Prediction</h1>
<p>由于软件处在不断地更新中，想要完全匹配 stack trace 是几乎不可能的，如 Table 2 所示。因此不可能采用查表的方式实现 lifetime prediction，作者提出使用一个 LSTM 模型来实现 stack trace 到 lifetime 的映射。作者通过 ML 模型的泛化能力，解决了Coverage 和 Instability 问题。同时使用 cache 对预测结果进行缓存，将模型预测的 overhead 平摊到整个运行周期内。<br>
在数据集方面，则同一软件的多个不同版本进行 sample 以构建数据集，让 ML 模型学习更多特征。</p>
<h1 id="4-sampling-based-data-collection">4. Sampling-based Data Collection</h1>
<p>使用 continuous profiling tools 对 Server 收集样本，每个样本由 alloc 和 free 时的 stack trace，objcet size 及 object address 组成。<br>
主要是使用了一个 HTTP handler，在 alloc 和 free 函数中加入 hook，收集 stack trace 等信息。这一部分的工作和 profile 高度相关，需要后续补充一些背景知识。</p>
<h1 id="5-lifetime-prediciton-model">5. Lifetime Prediciton Model</h1>
<h2 id="51-data-processing">5.1 Data Processing</h2>
<p>在本文的方案中，将对象的寿命按对数分为以下几个等级（lifetime classes，LC）：1ms, 10ms, 100ms, 1s, 10s, 100s, &gt; 100s。数据收集过程反复执行多次，以减小误差。用同一个对象寿命的 95% 分位数向以上的 LC 取整，作为这个对象的寿命的预测值，即模型的预期输出。对象 alloc 时的 stack trace 则作为模型的输入。<br>
同时，对在数据收集过程中出现频次比较高的 stack trace，相应地增加其在数据集中出现的比重。最终整个数据集约有数万个样本。</p>
<h2 id="52-machine-learning-model">5.2 Machine Learning Model</h2>
<p><img src="https://windchang1999.github.io/post-images/1628479728248.png" alt="" loading="lazy"><br>
<img src="https://windchang1999.github.io/post-images/1628479698284.png" alt="" loading="lazy"><br>
模型的输入为 alloc 时经过 tokenize 的 stack trace。从上到下处理 stack trace 就代表了 alloc 发生时的上下文，这种处理模式正好是 RNN 的长项。出于 overhead 的考虑，作者使用了 LSTM。</p>
<h2 id="53-model-implementation">5.3 Model Implementation</h2>
<p>用 Tensorflow 构建和训练模型。在训练完成后，使用 TF XLA compiler 将模型编译为 C++ 代码，最后和 allocator 链接在一起，降低调用开销。</p>
<h1 id="6-lifetime-aware-allocator-design">6. Lifetime Aware Allocator Design</h1>
<p>与基于 size classes 管理的 allocator 不同，LLAMA 直接基于 predict lifetime 进行管理。在设计过程中，存在以下挑战：</p>
<ol>
<li>怎么尽可能降低内部碎片？</li>
<li>怎么处理 misprediction？</li>
<li>怎么降低 prediction 带来的 latency？</li>
</ol>
<h2 id="61-heap-structure-and-concurrency">6.1 Heap Structure and Concurrency</h2>
<p>在 space 尺度上：<br>
LLAMA 将一个 huge page （2MB）划分为 8kB 的 block，block 进一步划分为 128B 的line。<br>
huge page 由 global allocator （per-process）向 OS 申请并管理，当用户程序申请的空间大于1个block的大小时，由 global allocator 直接分配连续的空闲block；为了提高并发性能，还存在着 thread-local allocator，用于分配小于1个block大小的空间。thread-local allocator 向 global allocator 请求 block span （即多个连续的block），再进一步将 block 划分为 lines 进行管理。<br>
在变量的寿命尺度上：</p>
<ol>
<li>同一个 block span 里的 object 的寿命预测值相同</li>
<li>每个 huge page 都有一个 LC 值，huge page 内部 block 的 LC 值应该小于等于 huge page 的 LC 值</li>
</ol>
<h2 id="62-lifetime-based-huge-page-management">6.2 Lifetime-Based Huge Page Management</h2>
<h3 id="621-huge-page-和-block-的状态转移">6.2.1 Huge Page 和 Block 的状态转移</h3>
<p>在 huge page 上发生的第一个 alloc 的 object 的 LC 即为该 huge page 的 LC 值。<br>
huge page 有三种可能的状态：open，active 和 free，其中 open 和 active 表明一个 huge page 正在使用。</p>
<ol>
<li>open 状态表示 huge page 处于第一次 alloc 到 huge page 完全填满的状态之间，在该状态下 LLAMA 认为该 huge page 是较为规整的，只会向 huge page 中分配与 huge page LC 值相同的 object。在 open 的状态下分配的 block 称为 residual block</li>
<li>在第一次把 huge page 分配完毕后，huge page 会从 open 转为 active 状态，并根据 huge page 的 LC 值设置一个 deadline。在进入 active 状态后，不会再退回 open 状态，只会在所有 block 都被 free 之后进入 free 状态。在 active 状态下分配的 block 称为 non-residual block</li>
<li>huge page 中的所有 block 都已经被释放后，则进入 free 状态，此时 global allocator 会把整个 huge page 还给 OS</li>
</ol>
<h3 id="622-初始分配逻辑">6.2.2 初始分配逻辑</h3>
<p><img src="https://windchang1999.github.io/post-images/1628491448571.png" alt="" loading="lazy"><br>
如 Figure 6(a) 所示，LLAMA 首先会把相同 LC 值的 block 分配到 open 的 huge page 中，这些 block 具有和 huge page 相同的 LC 值。填满的 huge page 从 open 状态转为 active 状态。经过一段时间后，如 Figure 6(b) 所示，有一部分 block 会被释放，在 active 状态的 huge page 中留下空洞。</p>
<h2 id="63-limiting-fragmentation-by-recycling-blocks">6.3 Limiting Fragmentation by Recycling Blocks</h2>
<p>在后续进一步分配 blokc 时，LLAMA 会重复利用 6.2.2 中留下的空洞。首先，如果新分配的 block 的 LC 值比较小，则会从 LC 值最大且处在 active 状态的 huge page 中找一个 free block 来分配。如果 LC 值最大且处在 active 状态的 huge page 没有空位，则从 LC 值次大的处在 active 状态的 huge page 中寻找，依次类推。如果直到最后也没有找到 LC 值较大且处在 active 状态的空闲 block，则从 LC 值相等且处在 open 状态的 huge page 中分配一个 block。<br>
<img src="https://windchang1999.github.io/post-images/1628492230761.png" alt="" loading="lazy"><br>
如 Figure 6(c) 所示，新分配了数个 LC 值为 10ms 的 block，则从 LC 值高且处在 active 状态的 huge page 中逐级往下分配。这样一来有以下几点好处：</p>
<ol>
<li>将 LC 小的 object 分配在 LC 大的 huge page 中，在 ML 模型预测结果准确或者偏大的情况下，能够充分利用空间。这些 object 很有可能在其他 block free 之前得到释放，能够反复利用，降低 memory footprint</li>
<li>如果模型预测值偏低，那么这些处在大 LC huge page 中的 block 有更多时间来释放，不会影响系统的性能，因此提供了一定的容错性</li>
</ol>
<h2 id="64-tolerating-prediction-errors">6.4 Tolerating Prediction Errors</h2>
<p><img src="https://windchang1999.github.io/post-images/1628492939610.png" alt="" loading="lazy"><br>
在系统运行过程中，不可避免的可能出现以下两种情况：</p>
<ol>
<li>该 huge page 的 LC 值偏低，其中的 block 的寿命超过了页 LC 值。例如 Figrue 6(d) 中的 page 1，其中两个 block 的预测寿命是 10ms，但实际寿命可能是 100ms，导致碎片的增加。</li>
<li>该 huge page 的 LC 值偏高，其中的 block 的寿命远远达不到页 LC 值。这种情况出现在 page 9。在高 LC 值的 huge page 中为了限制碎片会向其中填入低 LC 值的 block，导致整个页会同时存在多种 LC 值的 block，也会导致碎片增加。<br>
因此，需要动态调节 huge page 的 LC 值，调节的触发条件是 huge page 在其 deadline 前，内部仍有 block 未释放。在 huge page 从 open 转向 active 时，LLAMA 会按照如下公式计算该 huge page 的 deadline ：</li>
</ol>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mo>=</mo><mi>c</mi><mi>u</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>m</mi><mi>p</mi><mo>+</mo><mi>K</mi><mo>∗</mo><mi>L</mi><msub><mi>C</mi><mrow><mi>h</mi><mi>u</mi><mi>g</mi><mi>e</mi><mi>p</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">deadline = current\_timestamp + K * LC_{hugepage}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">L</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中 K 取 2 或 4（取离 10 和 1 比较远的数）。当 ddl 截止时如果 huge page 仍未能释放，说明存在着 misprediction。此时需要根据 huge page 内部 block 是否为 residual block 来判断能否对 LC 值进行调整。</p>
<ol>
<li>针对第一种情况，如果 huge page 中全为 residual block，则表明每个 block 的寿命都要大于 huge page 的 LC 值，可以将 huge page 升高一档</li>
<li>针对第二种情况，如果 huge page 中全为 non-residual block，则表明每个 block 的寿命都小于 huge page 的 LC 值，可以将 huge page 降低一档<br>
（可以认为 residual block 标记了同一 huge page 中最大的 block 的寿命值）</li>
</ol>
<h2 id="65-data-structures-66-recycling-lines-in-block-spans">6.5 Data Structures &amp; 6.6 Recycling Lines in Block Spans</h2>
<p>这一部分内容涉及细粒度的内存管理，本文中跳过该两小节</p>
<h1 id="7-low-latency-and-accurate-prediction">7. Low-Latency and Accurate Prediction</h1>
<p><img src="https://windchang1999.github.io/post-images/1628496908548.png" alt="" loading="lazy"><br>
为了使得 lifetime prediction 具有实际意义，必须采取加速手段降低运行过程的 overhead，否则不如选择带有 GC 的语言对服务器进行编程。作者提出使用 Cache 进行加速，因为即使是记录 stack trace 其运行时间也是远远超过 alloc 过程的，所以 cache 的输入是 hash(stack height | return address | object size)，输出是变量的寿命预测值。只有当 cache miss 时，才记录 stack trace 并运行 ML 模型更新 cache。<br>
此方案的问题在于，如果有 alloc 过程相同，但寿命不同的变量存在，那么会提高 misprediction 的比率，虽然通过 LLAMA 的机制能够作出容错操作，但一定程度上还是降低了系统性能。因此还添加了经过一定的时间内 flush 整个 Cache 的操作。</p>
<h1 id="8-evaluation">8. Evaluation</h1>
<h2 id="81-进程碎片量">8.1 进程碎片量</h2>
<p><img src="https://windchang1999.github.io/post-images/1630655255243.png" alt="" loading="lazy"><br>
Live：进程中有意义的变量占用的内存空间<br>
碎片量 = TCMalloc || LLAMA - Live<br>
<img src="https://windchang1999.github.io/post-images/1630655380834.png" alt="" loading="lazy"><br>
可以看到 LLAMA 有相当不错的碎片降低效果</p>
<h2 id="82-预测准确率">8.2 预测准确率</h2>
<p><img src="https://windchang1999.github.io/post-images/1630655492100.png" alt="" loading="lazy"><br>
Figure 10(a) 说明只要抽样比率达到1:100000，也能保持有80%的准确率<br>
Figure 10(b) 展示了模型的泛化能力，在一个 Server Application 上训练的版本能够有效泛化到其他版本的二进制上</p>
<h2 id="83-latency">8.3 Latency</h2>
<p><img src="https://windchang1999.github.io/post-images/1630655549372.png" alt="" loading="lazy"><br>
Cache hit rate = 95%（测量值），fast path avg. = 53.6ns，global allocator avg. = 90.79ns。<br>
总体来看比 TCMalloc 慢 2-3 倍，作者认为加速 LLAMA 需要从 Lock 下手，解除同步过程中带来的性能损失</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ON-DEMAND-FORK: A Microsecond Fork for Memory-Intensive and Latency-Sensitive Application]]></title>
        <id>https://windchang1999.github.io/post/on-demand-fork-a-microsecond-fork-for-memory-intensive-and-latency-sensitive-application/</id>
        <link href="https://windchang1999.github.io/post/on-demand-fork-a-microsecond-fork-for-memory-intensive-and-latency-sensitive-application/">
        </link>
        <updated>2021-08-01T02:02:22.000Z</updated>
        <summary type="html"><![CDATA[<p>内存密集场景下的一中 fork 的优化实现。EuroSys'21, from Purdue University</p>
]]></summary>
        <content type="html"><![CDATA[<p>内存密集场景下的一中 fork 的优化实现。EuroSys'21, from Purdue University</p>
<!-- more -->
<h1 id="1-introduction">1. Introduction</h1>
<p>在通常的 fork 实现中，通过 COW 机制避免了父进程所有物理页的复制，但仍然需要将进程相关的内核状态全部复制，例如页表、文件描述符及 namespace。on-demand-fork 将 COW 机制拓展到最后一级页表，避免了内存密集型应用在 fork 过程中复制最后一级页表的开销。</p>
<h1 id="2-background-motivation">2. Background &amp; Motivation</h1>
<p>在快照（Redis），测试（AFL）及虚拟机测试等内存密集型场景下，执行 fork 系统调用需要花费大量的时间。当前 fork 的实现，存在以下两点不足：</p>
<ol>
<li>fork 复制的页表的数量与其所占用的内存大小约成正比。复制大量页表会带来一定程度的延迟，例如在 1GB 内存占用延迟为 6.5ms，50GB 内存占用延迟为 253.9ms。</li>
<li>在多个进程并发使用 fork 系统调用时，由于需要原子性地更新物理页的引用计数，因此存在竞争关系，并发拓展性较差。例如3个进程同时执行 fork 时 1GB 内存占用对应的延迟为 22.4 ms。<br>
<img src="https://windchang1999.github.io/post-images/1627784999191.png" alt="" loading="lazy"><br>
一种解决方案是使用 huge page，但其存在内部碎片大，缺页中断开销大，并且会降低子进程和父进程共享物理页的机会等缺陷。这些特性都会增加程序的 tail latency。<br>
在 on-demand-fork 过程中，子进程与父进程共享最后一级页表，将最后一级页表的复制工作，以及物理页引用计数的更新，推迟到需要写入内存或是需要修改进程的虚拟内存空间时执行。从而降低 fork 调用的延迟，并提高并发性能。</li>
</ol>
<h1 id="3-implementation">3. Implementation</h1>
<figure data-type="image" tabindex="1"><img src="https://windchang1999.github.io/post-images/1627787173054.png" alt="" loading="lazy"></figure>
<h2 id="31-overview">3.1 Overview</h2>
<p>修改内容：</p>
<ol>
<li>为最后一级页表添加一个引用计数器</li>
<li>修改 page fault handler，除了复制对应的数据物理页，还需要复制最后一级页表，并修改相关页表及数据物理页的引用计数器</li>
<li>修改物理页的 Free 逻辑，对一个引用计数为1的物理页的 free 操作，必须检查其页表的引用计数</li>
<li>修改 munmap、mremap 等函数的逻辑，在改变进程的虚拟地址空间映射时，如果涉及到的最后一级页表处在共享状态，则必须对最后一级页表采取相应的操作</li>
</ol>
<h2 id="32-fork-过程中页表复制流程">3.2 fork 过程中页表复制流程</h2>
<p>如上图，在子进程中建立前三级页表，第三级页表的内容则复制于父进程，使得 PMD 指向同一个第四级页表，更新第四级页表的引用计数器。通过 MMU 提供的 hierachical attribute 特性，将第三级页表 PMD 的 writeable 位置 0 来标记整个第四级页表为不可写状态。<br>
在 on-demand-fork 过程中不涉及到物理页的引用计数的修改，因此提高了并发性能。</p>
<h2 id="33-对内存写引起-page-fault-的流程">3.3 对内存写引起 Page Fault 的流程</h2>
<p>如果共享最后一级页表的进程执行了写操作，那么 PMD 中的 writeable 位则会引起 page fault。在中断处理程序中，需要：</p>
<ol>
<li>复制最后一级页表，更新 PMD 指向新的最后一级页表，降低其引用计数</li>
<li>复制数据物理页，更新 PTE 指向新的数据物理页<br>
<img src="https://windchang1999.github.io/post-images/1627878119864.png" alt="" loading="lazy"></li>
</ol>
<h2 id="34-unmapping-or-remapping-vmas">3.4 Unmapping or Remapping VMAs</h2>
<p>Unmapping 和 Remapping 需要修改进程的页表内容，因此如果修改过程中发现共享的最后一级页表，那么则需要执行相应的操作。以 munmap 为例：</p>
<ol>
<li>如果进程中仅有 1 个 VMA 使用了共享的最后一级页表，那么可以直接断开 PMD 到 PTE 的连接，降低最后一级页表的引用计数</li>
<li>如果进程中有多个 VMA 使用了共享的最后一级页表，那么则需要对最后一级页表执行 COW 操作<br>
如何检测两个 VMA 是否使用了同一个最后一级页表？最后一级页表映射了 page_size * NR_PTEs 的空间大小，比较两个 VMA 的地址的高位即可。</li>
</ol>
<h1 id="4-evaluation">4. Evaluation</h1>
<h2 id="40-setup">4.0 Setup</h2>
<p>CPU：16-core AMD EPYC 7302P CPU<br>
Memory：256 GB<br>
Linux Kernel Version：5.6.19</p>
<h2 id="41-microbenchmarks">4.1 Microbenchmarks</h2>
<h3 id="411-system-call-latency">4.1.1 System Call Latency</h3>
<p><img src="https://windchang1999.github.io/post-images/1627823295825.png" alt="" loading="lazy"><br>
因为不需要在 fork 过程中复制最后一级页表，因此 on-demand-fork 的执行时间更短。在 1GB 内存占用时所需时间约为 1/65，在 50GB 内存占用时所需时间约为 1/270。而 on-demand-fork 比使用了 huge page 的 fork 更快还与 huge page 需要在 fork 过程中给 PMD 加锁的原因。</p>
<h3 id="412-page-fault-handling-overhead">4.1.2 Page Fault Handling Overhead</h3>
<p><img src="https://windchang1999.github.io/post-images/1627824064954.png" alt="" loading="lazy"><br>
on-demand-fork 的缺页中断执行时间约为通常 fork 的 5.3 倍，后续的原因需要思考一下。因为在中断处理程序中只是需要多复制一个最后一级页表，以及需要设置最后一级页表的引用计数。可能与引用计数器需要使用原子指令进行操作有关。<br>
on-demand-fork 的缺页中断执行时间比使用 huge page 的 fork 短是自然的，毕竟 huge page 的缺页中断需要复制 2MB 的物理页，是 4KB 物理页的 512 倍。</p>
<h3 id="413-overall-performance">4.1.3 Overall Performance</h3>
<p><img src="https://windchang1999.github.io/post-images/1627824495354.png" alt="" loading="lazy"><br>
在本次测试中，应用占用了 50GB 的内存。<br>
访问内存的区域越多，访存操作中写操作越多，则在 on-demand-fork 中需要执行更多的最后一级页表及物理页的 COW 操作，摊平了on-demand-fork 带来的加速效果。在最坏情况，即需要写入所有的内存区域时，则几乎没有加速效果（仅 4%）。</p>
<h2 id="42-real-world-applications">4.2 Real-world Applications</h2>
<h3 id="421-fuzzing-afl">4.2.1 Fuzzing: AFL</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825367579.png" alt="" loading="lazy"><br>
吞吐量约有 2.2 倍的提升。</p>
<h3 id="422-unit-testing-sqlite">4.2.2 Unit Testing: SQLite</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825450356.png" alt="" loading="lazy"><br>
较大程度地降低了 fork 过程所使用的时间，使得主要开销移动到测试用例的运行中。</p>
<h3 id="423-snapshots-redis">4.2.3 Snapshots: Redis</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825534681.png" alt="" loading="lazy"><br>
latency 更低，波动更小，且对 tail latency 降低明显。</p>
<h3 id="424-triforceafl-vm-cloning">4.2.4 TriforceAFL: VM Cloning</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825589905.png" alt="" loading="lazy"><br>
大约同 4.2.1。</p>
<h3 id="425-apache-http-server-on-demand-fork-不能加速的例子">4.2.5 Apache HTTP Server （on-demand-fork 不能加速的例子）</h3>
<p><img src="https://windchang1999.github.io/post-images/1627825634742.png" alt="" loading="lazy"><br>
因为 HTTP Server 并非内存大户（在测试中 Apache 仅用了 7MB 内存），因此 fork 过程中的瓶颈并不是页表的复制，因此 on-demand-fork 和 fork 的表现没有明显的区别。</p>
<h1 id="5-不足之处">5. 不足之处</h1>
<ol>
<li>论文中对 fork 的并发性能做了测试（3个相同的进程同时执行 fork），但没有对 on-demand-fork 做相同的测试</li>
<li>在 on-demand-fork 过程中只针对页表进行了 COW 操作。是否可以找一些其他的场景，将 COW 操作运用到 fork 过程中需要复制的其他内核变量上？</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Linux in Unikernel Clothing]]></title>
        <id>https://windchang1999.github.io/post/a-linux-in-unikernel-clothing/</id>
        <link href="https://windchang1999.github.io/post/a-linux-in-unikernel-clothing/">
        </link>
        <updated>2021-07-23T01:20:08.000Z</updated>
        <summary type="html"><![CDATA[<p>如何把 Linux 定制得和 Unikernel 一般？EuroSys'20, from UIUC &amp; IBM。</p>
]]></summary>
        <content type="html"><![CDATA[<p>如何把 Linux 定制得和 Unikernel 一般？EuroSys'20, from UIUC &amp; IBM。</p>
<!-- more -->
<h1 id="1-introduction">1. Introduction</h1>
<p>该团队针对仿照 Unikernel 的云计算场景，通过：</p>
<ul>
<li>Kconfig 系统对 Linux Kernel 做定制裁剪</li>
<li>借助 Kernel Mode Linux 补丁让用户态程序能够运行在内核态来减小系统调用的上下文切换开销<br>
最终实现了与 Unikernel 性能相近的一个 Linux 内核发行版，并且拥有更高的兼容性，鲁棒性及更低的应用开发成本。</li>
</ul>
<h1 id="2-background-motivation">2. Background &amp; Motivation</h1>
<p>在理想的微服务的场景下，每个虚拟机实例只运行单个进程，Unikernel 是针对微服务场景的一种高性能、低资源占用、高安全性的应用部署方案。Unikernel 的核心思想是通过 LibOS 的形式向用户态应用程序提供网络、文件系统、硬件驱动等操作系统服务，并将用户态程序与 LibOS 链接为单个二进制。<br>
对于当前的 Unikernel 技术，作者主要分成两类：</p>
<ol>
<li>Language-based<br>
特点是需要采用与 LibOS 一致的编程语言编写应用程序。比如最早的 Unikernel 框架 MirageOS 要求开发者使用 OCaml 语言，IncludeOS 框架则要求使用 C++ 语言。因此这种方案有可能存在需要用特定编程语言重写软件的可能。</li>
<li>POSIX-like<br>
特点是需要重新实现一套 POSIX 标准，缺点在于工程量太大，实现难度高，兼容性差，而且往往在重新实现 Linux （至少在重新实现一部分 Linux）。虽然有部分框架例如 Rumprun 重用了 NetBSD 的代码，减小了工程量，提高了兼容性，但存在定制程度不够高的问题。<br>
这两种方案各有优缺点，但作者认为统一的问题在于无法重用 Linux 及开源社区社区多年来的成果，重复造轮子，并且部署难度高（Language-based）或者是框架开发难度高（POSIX-like），兼容性无法保证。<br>
作者认为理想的应用部署方案应该既有 Unikernel 的高性能、低资源占用特性，又能够良好的与开源社区对接，针对以上痛点，因此提出了 Linux 高度定制这一条路线。</li>
</ol>
<h1 id="3-method">3. Method</h1>
<p><img src="https://windchang1999.github.io/post-images/1627217321610.png" alt="" loading="lazy"><br>
整体而言，本文中的方案有两部分工作，分别为：</p>
<ul>
<li>Specialization：通过对内核进行高度的定制、裁剪，同时定制 init 脚本及部分 rootfs 的内容，减小生成二进制的大小，提高启动速度。</li>
<li>System call overhead elimination：对内核和 musl libc 添加 KML 补丁，保证内核和应用程序都运行在内核态上，以此减小系统调用引起的上下文切换的开销。<br>
在本方案中，Linux Kernel 采用 4.0 版本，rootfs 则从 Docker Hub 中获取。</li>
</ul>
<h2 id="31-specialization">3.1 Specialization</h2>
<p><img src="https://windchang1999.github.io/post-images/1627218849240.png" alt="" loading="lazy"><br>
<img src="https://windchang1999.github.io/post-images/1627217796115.png" alt="" loading="lazy"><br>
本文中针对内核的裁剪在 Kconfig 的导引下完成。作者并没有遍历16000多个 Linux 配置项，而是从 AWS FIrecracker 的 microVM 的833个配置项开始，逐个地手动剔除配置项，最终剩下283个配置项，构成了 lupine-base config。<br>
在剔除出的配置项中，作者将其分为三类，分别是：</p>
<ol>
<li>Application speicific：例如某些系统调用，一些比较小众的网络协议及文件系统，一些压缩及加密算法实现，Debugging-related 等</li>
<li>Multiprocessing &amp; 安全相关：例如 SMP/NUMA，cgroup/namespace；SELINUX，KPTI（kernel page table isolation）</li>
<li>HW management：driver，arch-specific，power-management<br>
作者认为2、3属于在 Unikernel 场景下属于不必要的配置项，而1中配置项则需要根据部署的应用不同，按需开启。利用 lupine-base config 加上适当的 Application speicific 的配置项编译内核，即完成内核的定制。<br>
针对 init 脚本的定制在本文中没有详细描述，但大致是根据 kernel 定制的情况，按需屏蔽一些系统服务的启动流程。</li>
</ol>
<h2 id="32-eliminating-system-call-overhead">3.2 Eliminating System Call Overhead</h2>
<p>主要是使用了 Kernel Mode Linux 补丁，这是一个现成的项目，其主要作用是允许原来运行在用户态的应用程序运行在内核态。原有的 KML 适用于多进程环境，仅有一部分进程运行在内核态（在 /trusted 目录下），而剩下的进程仍然运行在用户态。由于在 Unikernel 场景下，仅需要运行单个进程，所以作者对 KML 进行了修改，使所有用户态程序都运行在内核态下。<br>
此外，作者还针对 musl libc （的二进制）进行了小量的修改，主要是将原有的系统调用指令替换为普通的函数调用指令，而 kernel 的系统调用函数符号通过 vsyscall 方法导出。</p>
<h1 id="4-evaluation">4. Evaluation</h1>
<p>在性能测试部分，作者主要给出三个结论：</p>
<ol>
<li>内核的裁剪十分重要对比于 microVM，image size 减小 73%，启动时间提高 59%，内存占用减小 28%，吞吐量提高 33%；但是针对每个应用程序对内核做进一步的定制相比于 lupine-general 仅有4%的性能提升。只需要在 lupine-base 的基础上加入19个配置项就能运行 Docker Hub 上的 top 20 应用程序。</li>
<li>系统调用的额外开销不是 Unikernel 的主要性能瓶颈</li>
<li>相比于 POSIX-like 的 Unikernel，Lupine 拥有更好的泛用性，应用程序无需修改源码即可直接运行，并且能够重用 Linux 及 glibc 中高度优化的代码</li>
</ol>
<h2 id="41-configuration-diversity">4.1 Configuration Diversity</h2>
<p><img src="https://windchang1999.github.io/post-images/1627264288724.jpg" alt="" loading="lazy"><br>
上图展示了为了在 Lupine Linux 上运行 Docker Hub top 20 软件，所需要向 lupine-base 中加入的配置项的数量。<br>
本文中衡量软件是否正常运行的标准比较 ad hoc，主要是逐个加入配置项，然后观察软件的运行状态（例如基本功能是否正常，日志是否有异常输出等）。<br>
<img src="https://windchang1999.github.io/post-images/1627264539058.jpg" alt="" loading="lazy"><br>
上图说明为了运行 Docker Hub top 20 软件，只需要向 lupine-base 中加入 19 个配置项即可，作者定义 lupine-base + 这19个配置项 = lupine-general。因此针对单个软件定制的内核实际上和 lupine-general 仅仅只差了几个配置项，理论上性能不应该有太大的差距，后续环节也证明了这一结论。</p>
<h2 id="42-image-size">4.2 Image Size</h2>
<p><img src="https://windchang1999.github.io/post-images/1627264868930.jpg" alt="" loading="lazy"><br>
上图中只显示了 Kernel image size，lupine-base 的内核体积约 4MB。但这里都是和 POSIX-like 的 unikernel 来做对比，实际上 language-based 的 unikernel 能做到非常小，例如 MirageOS 框架编写的 DNS 服务器，内核+应用程序的总大小仅为 184KB。</p>
<h2 id="43-boot-time">4.3 Boot Time</h2>
<p><img src="https://windchang1999.github.io/post-images/1627265302276.jpg" alt="" loading="lazy"><br>
启动时间为 30ms 左右，这里作者使用的 lupine-nokml 是没有安装 KML 补丁的内核源码 + 开启了 CONFIG_PARAVIRT 配置项的变种，主要问题是 CONFIG_PARAVIRT 能够在 AWS Firecracker 环境下提高启动速度，但是会和 KML 补丁冲突，在后续环节可以看到是否安装 KML 对性能的影响不大。在使用 lupine-base 设置时启动时间会达到 71ms，比 microVM（开启了 CONFIG_PARAVIRT）更高。</p>
<h2 id="44-memory-footprint">4.4 Memory Footprint</h2>
<p><img src="https://windchang1999.github.io/post-images/1627265824051.jpg" alt="" loading="lazy"><br>
主要测试方法是逐渐降低 Firecracker 给 VM 提供的内存，直到系统工作异常。比较有趣的一点是 Linux-base 的 VM（如 microVM 和 lupine），无论运行何种软件占用的内存都是一个恒定值，这与 Linux 内核的内存 lazy-allocation 有关。</p>
<h2 id="45-system-call-latency-kml-improvement">4.5 System Call Latency &amp;&amp; KML improvement</h2>
<p><img src="https://windchang1999.github.io/post-images/1627266322347.jpg" alt="" loading="lazy"><br>
测试中的 null 表示 getpid 系统调用，read 表示对 /dev/zero 进行读操作，write 表示对 /dev/null 进行写操作。lupine-base 的 system call latency 明显更低，但有一点需要思考：为什么仅仅对内核进行剪裁（lupine-nokml 与 microVM 对比）能够明显降低 read/write system call latency，对内核的剪裁应该不会改变 system call 的调用路径，作者原文中没有给出解释。个人猜测应该与文件系统的性能、安全相关功能的禁用有关。</p>
<h2 id="46-application-performance">4.6 Application performance</h2>
<p><img src="https://windchang1999.github.io/post-images/1627266782521.jpg" alt="" loading="lazy"><br>
相比于 microVM 均有 20% 左右的性能提升，作者将其归功于关闭了一些安全功能，提高了性能。此外关注以下两点：</p>
<ol>
<li>lupine 和 lupine-genreal 对比，表明在应用程序这个层次做定制性能提升不大</li>
<li>lupine 和 lupine-nokml 对比，表明 KML 对性能影响不大，主要性能提升来自内核定制</li>
<li>其他 POSIX-like 的 unikernel 的系统服务代码由于缺乏充分的优化，因此性能甚至和 microVM 有一定的差距</li>
</ol>
<h1 id="5-beyond-unikernels">5. Beyond Unikernels</h1>
<p>无论是 language-based 还是 POSIX-like 的 unikernel，都存在泛用性问题。例如不能支持 fork，不能支持多处理器调用，在一些情况下会给 Linux 应用的部署带来困难。作者在这一章中着重展示了 Lupine 拥有更好的泛用性。向 lupine-base 加入多进程相关配置项，即可开启多进程支持，并且验证了其额外开销较小的结论。<br>
<img src="https://windchang1999.github.io/post-images/1627270504573.png" alt="" loading="lazy"><br>
首先是 control process （例如 shell、deamon 等不会和应用程序抢占资源的进程）并不影响 system call latency，这表明加入多个地址空间支持对内核的影响并不大（没有向内核中增加安全增强功能大）。<br>
<img src="https://windchang1999.github.io/post-images/1627270708424.png" alt="" loading="lazy"><br>
其次作者比较了在没有加入多进程支持时单地址空间下使用多个pthread线程，在加入多进程支持后使用多个进程两种情况下的 messaging benchmark。结果表明加入多进程支持对相互竞争资源的进程性能也不会有太大的影响。理论上讲，在 Linux 中 pthread 库创建的线程也采用 task_struct 结构体作为控制块，两种情况的差异只是是否切换页表基地址寄存器及 flush TLB 而已。<br>
此外，作者还针对 SMP 配置做了一定的测试，打开内核的 SMP 功能后，在 sem_posix, futex 和 make -j 三种场景下，性能损失分别为 3%、8%、3%。</p>
<h1 id="6-discussion">6. Discussion</h1>
<ol>
<li>通过人工的方法分析 Kconfig 的各个配置项可能存在误分类，同时不同的配置项可能有着类似的功能，能够相互替换，因此 lupine-base 并不唯一且最小</li>
<li>如何自动化地、准确地通过应用程序的源码或者二进制得到所需的内核配置项，是接下来要继续研究的问题。对于 language-based 的 Unikernel 不存在这一问题</li>
<li>lupine 没有实现轻量化的 VMM</li>
<li>lupine 不能实现“immutable infrastructure”，在应用部署上存在局限性，并且相比于 language-based 的 unikerel 攻击面更大（有 rootfs，有 loader，应用程序与OS没有链接为一个整体）</li>
<li>相比于 lannguage-based 的 unikernel ，lupine 不能实现跨应用程序和OS的程序优化</li>
</ol>
<h1 id="参考资料">参考资料</h1>
<ol>
<li>https://github.com/cetic/unikernels：一个介绍 unikernels 的项目，总体看来 unikernel 的性能可能不如 container</li>
<li>https://learning.oreilly.com/library/view/unikernels/9781492042815/：电子书unikernls</li>
</ol>
]]></content>
    </entry>
</feed>