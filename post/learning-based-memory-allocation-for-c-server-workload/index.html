<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Learning-based Memory Allocation for C++ Server Workload | windchang1999&#39;s blog</title>
<link rel="shortcut icon" href="https://windchang1999.github.io/favicon.ico?v=1630655586721">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://windchang1999.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Learning-based Memory Allocation for C++ Server Workload | windchang1999&#39;s blog - Atom Feed" href="https://windchang1999.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="利用 LSTM 预测变量的寿命，结合新型的内存分配器缓解 C++ 服务器的 Huge Page 的内部碎片问题。ASPLOS'20, from Google &amp; CMU &amp; SBU。

1. Introduction
服务器..." />
    <meta name="keywords" content="论文笔记" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://windchang1999.github.io">
  <img class="avatar" src="https://windchang1999.github.io/images/avatar.png?v=1630655586721" alt="">
  </a>
  <h1 class="site-title">
    windchang1999&#39;s blog
  </h1>
  <p class="site-description">
    
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Learning-based Memory Allocation for C++ Server Workload
            </h2>
            <div class="post-info">
              <span>
                2021-08-08
              </span>
              <span>
                14 min read
              </span>
              
                <a href="https://windchang1999.github.io/tag/Fddl1awQ2/" class="post-tag">
                  # 论文笔记
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://windchang1999.github.io/post-images/learning-based-memory-allocation-for-c-server-workload.png" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <p>利用 LSTM 预测变量的寿命，结合新型的内存分配器缓解 C++ 服务器的 Huge Page 的内部碎片问题。ASPLOS'20, from Google &amp; CMU &amp; SBU。</p>
<!-- more -->
<h1 id="1-introduction">1. Introduction</h1>
<p>服务器的 memory footprint 变大，但 TLB 并没有随之增大，导致地址翻译的性能下降。一种解决方案是使用 huge page 来增加 TLB reach，以提高地址翻译性能。但缺乏管理的 huge page 几乎不可避免地会引入内部碎片，导致物理内存的浪费。<br>
作者指明了在长期运行的服务器中造成 huge page 的根本原因：即将寿命长短不一的对象分配于同一个 huge page 中。因为 C++ 的 allocator 并不会在运行过程中移动对象以整理碎片，当寿命较短的对象释放后，寿命长的对象仍然存在于 huge page 中，使得 huge page 被占用，留下大量的内部碎片，增加了整个服务器应用程序的内存占用量。<br>
long-lived object 占用 huge page 是十分常见的。即使假设 short-lived object 占总 object 数量的 99.99%，object size = 64 B 的情况下，在 2MB 的 huge-page 中至少存在一个 long-lived object 的概率则是 1 - (0.9999)^(2MB/64B) = 96%。因此使用 huge page 不可避免地会导致碎片问题。<br>
为了从根本上解决 huge page 的碎片问题，必须考虑以下要素：</p>
<ol>
<li>如何预测对象的寿命</li>
<li>如何尽可能将具有相似寿命的对象分配在同一个 huge page 中</li>
<li>在预测寿命存在偏差时，如何保证分配机制的效率<br>
同时，为了保证方案具有工程意义，还需要考虑运行的 overhead。</li>
</ol>
<h2 id="contribution">Contribution</h2>
<ol>
<li>使用 RNN 来对变量的寿命进行预测。该模型可以泛化到不同版本、不同构建方式的同一 Server Application 中</li>
<li>设计了 Learned Lifetime-Aware Memory Allocator （LLAMA），实现了基于 object lifetime classes 的 huge page 内部碎片的优化</li>
</ol>
<h1 id="2-motivation-background">2. Motivation &amp; Background</h1>
<h2 id="21-server-segmentation">2.1 Server Segmentation</h2>
<p><img src="https://windchang1999.github.io/post-images/1628427362253.png" alt="" loading="lazy"><br>
<img src="https://windchang1999.github.io/post-images/1628427369625.png" alt="" loading="lazy"><br>
在 Server 运行过程中其实 long-lived object 占比很少，但使用 2MB 的 huge page 仍然会造成近2.15倍的内存占用。这是因为在本节的例子中，Server 在处理一个 request 时，除了需要动态分配一系列临时对象用来处理数据外，还需要分配一些 long-lived 的变量来记录系统状态。在这样的情形下使用 huge page 会导致大量内存碎片。<br>
当下已经有很多针对 4KB 页的内部碎片解决方案，但不适宜迁移到 huge page 中；降低 huge page 的内部碎片是更有挑战性的问题。</p>
<h2 id="22-lifetime-prediction-challenges">2.2 Lifetime Prediction Challenges</h2>
<p>先前的工作通常只是通过精确匹配堆栈上下文来二分地预测变量的生命周期，即给出一个 stack trace，预测器输出该变量属于 long-lived or short-lived，粒度过粗。而 LLAMA 则以 lifetime classes（LC）的粒度来管理变量，即将变量的寿命以对数划分为 1ms, 10ms, 100ms, 1s ....</p>
<h3 id="221-overhead">2.2.1 Overhead</h3>
<p><img src="https://windchang1999.github.io/post-images/1628428571014.png" alt="" loading="lazy"><br>
在运行过程中记录 stack trace 是相当耗时的，往往超过分配内存所需要的时间，再加上 prediction 所需要的时间，在运行过程中执行 lifetime prediction 会带来大量的 overhead。</p>
<h3 id="222-coverage-and-accuracy">2.2.2 Coverage and Accuracy</h3>
<p>作者在这一段拿 Profile-Guide Optimization(PGO) 和 lifetime prediction 所需要的 profile data 相比，大概说明的意思是 lifetime prediction 需要记录 allocation 和 free 时的 stack trace，数据量相对于 PGO 更多。这里不太懂 PGO 的原理，先留个坑。</p>
<h3 id="223-instability">2.2.3 Instability</h3>
<p>由于存在地址随机化和不同的编译配置，同一代码相同位置的 stack trace 也有可能不同，想要精确匹配 stack trace 存在相当的难度。因此当前的 lifetime predictor 通常是 online 进行 profile，带来比较严重的 overhead 问题。</p>
<h1 id="3-overview-of-lifetime-prediction">3. Overview of Lifetime Prediction</h1>
<p>由于软件处在不断地更新中，想要完全匹配 stack trace 是几乎不可能的，如 Table 2 所示。因此不可能采用查表的方式实现 lifetime prediction，作者提出使用一个 LSTM 模型来实现 stack trace 到 lifetime 的映射。作者通过 ML 模型的泛化能力，解决了Coverage 和 Instability 问题。同时使用 cache 对预测结果进行缓存，将模型预测的 overhead 平摊到整个运行周期内。<br>
在数据集方面，则同一软件的多个不同版本进行 sample 以构建数据集，让 ML 模型学习更多特征。</p>
<h1 id="4-sampling-based-data-collection">4. Sampling-based Data Collection</h1>
<p>使用 continuous profiling tools 对 Server 收集样本，每个样本由 alloc 和 free 时的 stack trace，objcet size 及 object address 组成。<br>
主要是使用了一个 HTTP handler，在 alloc 和 free 函数中加入 hook，收集 stack trace 等信息。这一部分的工作和 profile 高度相关，需要后续补充一些背景知识。</p>
<h1 id="5-lifetime-prediciton-model">5. Lifetime Prediciton Model</h1>
<h2 id="51-data-processing">5.1 Data Processing</h2>
<p>在本文的方案中，将对象的寿命按对数分为以下几个等级（lifetime classes，LC）：1ms, 10ms, 100ms, 1s, 10s, 100s, &gt; 100s。数据收集过程反复执行多次，以减小误差。用同一个对象寿命的 95% 分位数向以上的 LC 取整，作为这个对象的寿命的预测值，即模型的预期输出。对象 alloc 时的 stack trace 则作为模型的输入。<br>
同时，对在数据收集过程中出现频次比较高的 stack trace，相应地增加其在数据集中出现的比重。最终整个数据集约有数万个样本。</p>
<h2 id="52-machine-learning-model">5.2 Machine Learning Model</h2>
<p><img src="https://windchang1999.github.io/post-images/1628479728248.png" alt="" loading="lazy"><br>
<img src="https://windchang1999.github.io/post-images/1628479698284.png" alt="" loading="lazy"><br>
模型的输入为 alloc 时经过 tokenize 的 stack trace。从上到下处理 stack trace 就代表了 alloc 发生时的上下文，这种处理模式正好是 RNN 的长项。出于 overhead 的考虑，作者使用了 LSTM。</p>
<h2 id="53-model-implementation">5.3 Model Implementation</h2>
<p>用 Tensorflow 构建和训练模型。在训练完成后，使用 TF XLA compiler 将模型编译为 C++ 代码，最后和 allocator 链接在一起，降低调用开销。</p>
<h1 id="6-lifetime-aware-allocator-design">6. Lifetime Aware Allocator Design</h1>
<p>与基于 size classes 管理的 allocator 不同，LLAMA 直接基于 predict lifetime 进行管理。在设计过程中，存在以下挑战：</p>
<ol>
<li>怎么尽可能降低内部碎片？</li>
<li>怎么处理 misprediction？</li>
<li>怎么降低 prediction 带来的 latency？</li>
</ol>
<h2 id="61-heap-structure-and-concurrency">6.1 Heap Structure and Concurrency</h2>
<p>在 space 尺度上：<br>
LLAMA 将一个 huge page （2MB）划分为 8kB 的 block，block 进一步划分为 128B 的line。<br>
huge page 由 global allocator （per-process）向 OS 申请并管理，当用户程序申请的空间大于1个block的大小时，由 global allocator 直接分配连续的空闲block；为了提高并发性能，还存在着 thread-local allocator，用于分配小于1个block大小的空间。thread-local allocator 向 global allocator 请求 block span （即多个连续的block），再进一步将 block 划分为 lines 进行管理。<br>
在变量的寿命尺度上：</p>
<ol>
<li>同一个 block span 里的 object 的寿命预测值相同</li>
<li>每个 huge page 都有一个 LC 值，huge page 内部 block 的 LC 值应该小于等于 huge page 的 LC 值</li>
</ol>
<h2 id="62-lifetime-based-huge-page-management">6.2 Lifetime-Based Huge Page Management</h2>
<h3 id="621-huge-page-和-block-的状态转移">6.2.1 Huge Page 和 Block 的状态转移</h3>
<p>在 huge page 上发生的第一个 alloc 的 object 的 LC 即为该 huge page 的 LC 值。<br>
huge page 有三种可能的状态：open，active 和 free，其中 open 和 active 表明一个 huge page 正在使用。</p>
<ol>
<li>open 状态表示 huge page 处于第一次 alloc 到 huge page 完全填满的状态之间，在该状态下 LLAMA 认为该 huge page 是较为规整的，只会向 huge page 中分配与 huge page LC 值相同的 object。在 open 的状态下分配的 block 称为 residual block</li>
<li>在第一次把 huge page 分配完毕后，huge page 会从 open 转为 active 状态，并根据 huge page 的 LC 值设置一个 deadline。在进入 active 状态后，不会再退回 open 状态，只会在所有 block 都被 free 之后进入 free 状态。在 active 状态下分配的 block 称为 non-residual block</li>
<li>huge page 中的所有 block 都已经被释放后，则进入 free 状态，此时 global allocator 会把整个 huge page 还给 OS</li>
</ol>
<h3 id="622-初始分配逻辑">6.2.2 初始分配逻辑</h3>
<p><img src="https://windchang1999.github.io/post-images/1628491448571.png" alt="" loading="lazy"><br>
如 Figure 6(a) 所示，LLAMA 首先会把相同 LC 值的 block 分配到 open 的 huge page 中，这些 block 具有和 huge page 相同的 LC 值。填满的 huge page 从 open 状态转为 active 状态。经过一段时间后，如 Figure 6(b) 所示，有一部分 block 会被释放，在 active 状态的 huge page 中留下空洞。</p>
<h2 id="63-limiting-fragmentation-by-recycling-blocks">6.3 Limiting Fragmentation by Recycling Blocks</h2>
<p>在后续进一步分配 blokc 时，LLAMA 会重复利用 6.2.2 中留下的空洞。首先，如果新分配的 block 的 LC 值比较小，则会从 LC 值最大且处在 active 状态的 huge page 中找一个 free block 来分配。如果 LC 值最大且处在 active 状态的 huge page 没有空位，则从 LC 值次大的处在 active 状态的 huge page 中寻找，依次类推。如果直到最后也没有找到 LC 值较大且处在 active 状态的空闲 block，则从 LC 值相等且处在 open 状态的 huge page 中分配一个 block。<br>
<img src="https://windchang1999.github.io/post-images/1628492230761.png" alt="" loading="lazy"><br>
如 Figure 6(c) 所示，新分配了数个 LC 值为 10ms 的 block，则从 LC 值高且处在 active 状态的 huge page 中逐级往下分配。这样一来有以下几点好处：</p>
<ol>
<li>将 LC 小的 object 分配在 LC 大的 huge page 中，在 ML 模型预测结果准确或者偏大的情况下，能够充分利用空间。这些 object 很有可能在其他 block free 之前得到释放，能够反复利用，降低 memory footprint</li>
<li>如果模型预测值偏低，那么这些处在大 LC huge page 中的 block 有更多时间来释放，不会影响系统的性能，因此提供了一定的容错性</li>
</ol>
<h2 id="64-tolerating-prediction-errors">6.4 Tolerating Prediction Errors</h2>
<p><img src="https://windchang1999.github.io/post-images/1628492939610.png" alt="" loading="lazy"><br>
在系统运行过程中，不可避免的可能出现以下两种情况：</p>
<ol>
<li>该 huge page 的 LC 值偏低，其中的 block 的寿命超过了页 LC 值。例如 Figrue 6(d) 中的 page 1，其中两个 block 的预测寿命是 10ms，但实际寿命可能是 100ms，导致碎片的增加。</li>
<li>该 huge page 的 LC 值偏高，其中的 block 的寿命远远达不到页 LC 值。这种情况出现在 page 9。在高 LC 值的 huge page 中为了限制碎片会向其中填入低 LC 值的 block，导致整个页会同时存在多种 LC 值的 block，也会导致碎片增加。<br>
因此，需要动态调节 huge page 的 LC 值，调节的触发条件是 huge page 在其 deadline 前，内部仍有 block 未释放。在 huge page 从 open 转向 active 时，LLAMA 会按照如下公式计算该 huge page 的 deadline ：</li>
</ol>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mo>=</mo><mi>c</mi><mi>u</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>m</mi><mi>p</mi><mo>+</mo><mi>K</mi><mo>∗</mo><mi>L</mi><msub><mi>C</mi><mrow><mi>h</mi><mi>u</mi><mi>g</mi><mi>e</mi><mi>p</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">deadline = current\_timestamp + K * LC_{hugepage}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">L</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中 K 取 2 或 4（取离 10 和 1 比较远的数）。当 ddl 截止时如果 huge page 仍未能释放，说明存在着 misprediction。此时需要根据 huge page 内部 block 是否为 residual block 来判断能否对 LC 值进行调整。</p>
<ol>
<li>针对第一种情况，如果 huge page 中全为 residual block，则表明每个 block 的寿命都要大于 huge page 的 LC 值，可以将 huge page 升高一档</li>
<li>针对第二种情况，如果 huge page 中全为 non-residual block，则表明每个 block 的寿命都小于 huge page 的 LC 值，可以将 huge page 降低一档<br>
（可以认为 residual block 标记了同一 huge page 中最大的 block 的寿命值）</li>
</ol>
<h2 id="65-data-structures-66-recycling-lines-in-block-spans">6.5 Data Structures &amp; 6.6 Recycling Lines in Block Spans</h2>
<p>这一部分内容涉及细粒度的内存管理，本文中跳过该两小节</p>
<h1 id="7-low-latency-and-accurate-prediction">7. Low-Latency and Accurate Prediction</h1>
<p><img src="https://windchang1999.github.io/post-images/1628496908548.png" alt="" loading="lazy"><br>
为了使得 lifetime prediction 具有实际意义，必须采取加速手段降低运行过程的 overhead，否则不如选择带有 GC 的语言对服务器进行编程。作者提出使用 Cache 进行加速，因为即使是记录 stack trace 其运行时间也是远远超过 alloc 过程的，所以 cache 的输入是 hash(stack height | return address | object size)，输出是变量的寿命预测值。只有当 cache miss 时，才记录 stack trace 并运行 ML 模型更新 cache。<br>
此方案的问题在于，如果有 alloc 过程相同，但寿命不同的变量存在，那么会提高 misprediction 的比率，虽然通过 LLAMA 的机制能够作出容错操作，但一定程度上还是降低了系统性能。因此还添加了经过一定的时间内 flush 整个 Cache 的操作。</p>
<h1 id="8-evaluation">8. Evaluation</h1>
<h2 id="81-进程碎片量">8.1 进程碎片量</h2>
<p><img src="https://windchang1999.github.io/post-images/1630655255243.png" alt="" loading="lazy"><br>
Live：进程中有意义的变量占用的内存空间<br>
碎片量 = TCMalloc || LLAMA - Live<br>
<img src="https://windchang1999.github.io/post-images/1630655380834.png" alt="" loading="lazy"><br>
可以看到 LLAMA 有相当不错的碎片降低效果</p>
<h2 id="82-预测准确率">8.2 预测准确率</h2>
<p><img src="https://windchang1999.github.io/post-images/1630655492100.png" alt="" loading="lazy"><br>
Figure 10(a) 说明只要抽样比率达到1:100000，也能保持有80%的准确率<br>
Figure 10(b) 展示了模型的泛化能力，在一个 Server Application 上训练的版本能够有效泛化到其他版本的二进制上</p>
<h2 id="83-latency">8.3 Latency</h2>
<p><img src="https://windchang1999.github.io/post-images/1630655549372.png" alt="" loading="lazy"><br>
Cache hit rate = 95%（测量值），fast path avg. = 53.6ns，global allocator avg. = 90.79ns。<br>
总体来看比 TCMalloc 慢 2-3 倍，作者认为加速 LLAMA 需要从 Lock 下手，解除同步过程中带来的性能损失</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#1-introduction">1. Introduction</a>
<ul>
<li><a href="#contribution">Contribution</a></li>
</ul>
</li>
<li><a href="#2-motivation-background">2. Motivation &amp; Background</a>
<ul>
<li><a href="#21-server-segmentation">2.1 Server Segmentation</a></li>
<li><a href="#22-lifetime-prediction-challenges">2.2 Lifetime Prediction Challenges</a>
<ul>
<li><a href="#221-overhead">2.2.1 Overhead</a></li>
<li><a href="#222-coverage-and-accuracy">2.2.2 Coverage and Accuracy</a></li>
<li><a href="#223-instability">2.2.3 Instability</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3-overview-of-lifetime-prediction">3. Overview of Lifetime Prediction</a></li>
<li><a href="#4-sampling-based-data-collection">4. Sampling-based Data Collection</a></li>
<li><a href="#5-lifetime-prediciton-model">5. Lifetime Prediciton Model</a>
<ul>
<li><a href="#51-data-processing">5.1 Data Processing</a></li>
<li><a href="#52-machine-learning-model">5.2 Machine Learning Model</a></li>
<li><a href="#53-model-implementation">5.3 Model Implementation</a></li>
</ul>
</li>
<li><a href="#6-lifetime-aware-allocator-design">6. Lifetime Aware Allocator Design</a>
<ul>
<li><a href="#61-heap-structure-and-concurrency">6.1 Heap Structure and Concurrency</a></li>
<li><a href="#62-lifetime-based-huge-page-management">6.2 Lifetime-Based Huge Page Management</a>
<ul>
<li><a href="#621-huge-page-%E5%92%8C-block-%E7%9A%84%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BB">6.2.1 Huge Page 和 Block 的状态转移</a></li>
<li><a href="#622-%E5%88%9D%E5%A7%8B%E5%88%86%E9%85%8D%E9%80%BB%E8%BE%91">6.2.2 初始分配逻辑</a></li>
</ul>
</li>
<li><a href="#63-limiting-fragmentation-by-recycling-blocks">6.3 Limiting Fragmentation by Recycling Blocks</a></li>
<li><a href="#64-tolerating-prediction-errors">6.4 Tolerating Prediction Errors</a></li>
<li><a href="#65-data-structures-66-recycling-lines-in-block-spans">6.5 Data Structures &amp; 6.6 Recycling Lines in Block Spans</a></li>
</ul>
</li>
<li><a href="#7-low-latency-and-accurate-prediction">7. Low-Latency and Accurate Prediction</a></li>
<li><a href="#8-evaluation">8. Evaluation</a>
<ul>
<li><a href="#81-%E8%BF%9B%E7%A8%8B%E7%A2%8E%E7%89%87%E9%87%8F">8.1 进程碎片量</a></li>
<li><a href="#82-%E9%A2%84%E6%B5%8B%E5%87%86%E7%A1%AE%E7%8E%87">8.2 预测准确率</a></li>
<li><a href="#83-latency">8.3 Latency</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://windchang1999.github.io/post/on-demand-fork-a-microsecond-fork-for-memory-intensive-and-latency-sensitive-application/">
              <h3 class="post-title">
                ON-DEMAND-FORK: A Microsecond Fork for Memory-Intensive and Latency-Sensitive Application
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank"> Gridea</a>
  <a class="rss" href="https://windchang1999.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
